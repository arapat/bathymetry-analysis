{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Validating and formatting\n",
    "\n",
    "These are the scripts that validate and format the raw data files for the bathymetry project.\n",
    "\n",
    "## Meaning of columns\n",
    "\n",
    "\n",
    "```\n",
    "LAT LONG DEPTH sigh sigd SID  pred id   d10   d20   d60    age   VGG   rate  sed    rough\n",
    "C1  C2   C3    C4   C5   C6   C7   C8   C9    C10   C11    C12   C13   C14   C15    C16\n",
    " \n",
    "G:T     kind   year\n",
    "C17     C18    C19\n",
    "\n",
    "NDP_2.5   NDP_5   NDP_10   NDP_30   STD_2.5m   STD_5m   STD_10m   STD_30m   MED_2.5m   MED_5m   MED_10m\n",
    "C20       C21     C22      C23      C24        C25      C26       C27       C28        C29      C30\n",
    "\n",
    "MED_30m\n",
    "C31\n",
    "\n",
    "DEPTH-MED_2.5m)/STD_2.5m     (DEPTH-MED_5m)/STD_5m       (DEPTH-MED_10m)/STD_10m      (DEPTH-MED_30m)/STD_30m\n",
    "C32                          C33                         C34                          C35\n",
    "\n",
    "\n",
    "VGG = Vertical gravity gradient (= curvature of the sea surface) \n",
    "rate = seafloor spreading rate\n",
    "rough = seafloor roughness\n",
    "G:T = Gravity:Topography ratio \n",
    "NDP = Number of data points\n",
    "MED = Median\n",
    "STD = Standard dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_BASE_DIR = \"/geosat2/julaiti/tsv_all\"\n",
    "TRAINING_FILES_DESC = os.path.join(DATA_BASE_DIR, \"training_files_desc.txt\")\n",
    "TESTING_FILES_DESC = os.path.join(DATA_BASE_DIR, \"testing_files_desc.txt\")\n",
    "VALIDATION_FILES_DESC = os.path.join(DATA_BASE_DIR, \"validation_files_desc.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(DATA_BASE_DIR, \"valid_regions.txt\")) as f:\n",
    "    input_dir = list(map(\n",
    "        lambda s: os.path.join(DATA_BASE_DIR, s.strip()), f.readlines()))\n",
    "len(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tLAT\n",
      "1\tLONG\n",
      "2\tDEPTH\n",
      "3\tsigh\n",
      "4\tsigd\n",
      "5\tSID\n",
      "6\tpred\n",
      "7\tid\n",
      "8\td10\n",
      "9\td20\n",
      "10\td60\n",
      "11\tage\n",
      "12\tVGG\n",
      "13\trate\n",
      "14\tsed\n",
      "15\trough\n",
      "16\tG:T\n",
      "17\tkind\n",
      "18\tyear\n",
      "19\tNDP_2.5\n",
      "20\tNDP_5\n",
      "21\tNDP_10\n",
      "22\tNDP_30\n",
      "23\tSTD_2.5m\n",
      "24\tSTD_5m\n",
      "25\tSTD_10m\n",
      "26\tSTD_30m\n",
      "27\tMED_2.5m\n",
      "28\tMED_5m\n",
      "29\tMED_10m\n",
      "30\tMED_30m\n",
      "31\tDEPTH-MED_2.5m)/STD_2.5m\n",
      "32\t(DEPTH-MED_5m)/STD_5m\n",
      "33\t(DEPTH-MED_10m)/STD_10m\n",
      "34\t(DEPTH-MED_30m)/STD_30m\n"
     ]
    }
   ],
   "source": [
    "columns = \"\"\"\n",
    "LAT LONG DEPTH sigh sigd SID  pred id   d10   d20   d60    age   VGG   rate  sed    rough\n",
    "G:T     kind   year\n",
    "NDP_2.5   NDP_5   NDP_10   NDP_30   STD_2.5m   STD_5m   STD_10m   STD_30m   MED_2.5m   MED_5m   MED_10m\n",
    "MED_30m\n",
    "DEPTH-MED_2.5m)/STD_2.5m     (DEPTH-MED_5m)/STD_5m       (DEPTH-MED_10m)/STD_10m      (DEPTH-MED_30m)/STD_30m\n",
    "\"\"\"\n",
    "\n",
    "columns = list(enumerate(map(lambda s: s.strip(), columns.strip().split())))\n",
    "for a, b in columns:\n",
    "    print(\"{}\\t{}\".format(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of cruises from each regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 408 records under '/geosat2/julaiti/tsv_all/3DGBR'.\n",
      "There are 103 records under '/geosat2/julaiti/tsv_all/AGSO'.\n",
      "There are 16 records under '/geosat2/julaiti/tsv_all/DNC'. They will *only* be used for testing.\n",
      "There are 3 records under '/geosat2/julaiti/tsv_all/IBCAO'. They will *only* be used for testing.\n",
      "There are 94 records under '/geosat2/julaiti/tsv_all/IFREMER'.\n",
      "There are 546 records under '/geosat2/julaiti/tsv_all/JAMSTEC'.\n",
      "There are 0 records under '/geosat2/julaiti/tsv_all/JAMSTEC2'. They will *only* be used for testing.\n",
      "There are 1375 records under '/geosat2/julaiti/tsv_all/NGA'.\n",
      "There are 24 records under '/geosat2/julaiti/tsv_all/NGA2'. They will *only* be used for testing.\n",
      "There are 14 records under '/geosat2/julaiti/tsv_all/NOAA'. They will *only* be used for testing.\n",
      "There are 250 records under '/geosat2/julaiti/tsv_all/SIO'.\n",
      "There are 618 records under '/geosat2/julaiti/tsv_all/US_multi'.\n",
      "There are 4655 records under '/geosat2/julaiti/tsv_all/NOAA_geodas'.\n",
      "There are 1045 records under '/geosat2/julaiti/tsv_all/NGDC'.\n",
      "There are 50 records under '/geosat2/julaiti/tsv_all/CCOM'. They will *only* be used for testing.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6374, 1360, 1467)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import io\n",
    "\n",
    "\n",
    "def is_final_tsv_file(filepath):\n",
    "    # if \"SIO\" in filepath:\n",
    "    #     return filepath.endswith(\".tsv_all_final\")\n",
    "    return filepath.endswith(\".tsv_all\")\n",
    "\n",
    "\n",
    "def is_enough_cols(filepath):\n",
    "    with io.open(filepath, 'r', newline='\\n') as f:\n",
    "        return len(f.readline().split()) == len(columns)\n",
    "\n",
    "training_files = []\n",
    "testing_files = []\n",
    "validation_files = []\n",
    "for root in input_dir:\n",
    "    all_files = map(lambda s: os.path.join(root, s), os.listdir(root))\n",
    "    valid_files = filter(is_final_tsv_file, all_files)\n",
    "    valid_files = filter(is_enough_cols, valid_files)\n",
    "\n",
    "    filepath = list(valid_files)\n",
    "    shuffle(filepath)\n",
    "    if len(filepath) <= 50:\n",
    "        print(\"There are {} records under '{}'. They will *only* be used for testing.\".format(len(filepath), root))\n",
    "        testing_files += filepath\n",
    "    else:\n",
    "        thr1 = int(len(filepath) * 0.15)\n",
    "        thr2 = thr1 + thr1  # another 0.2\n",
    "        testing_files += filepath[:thr1]\n",
    "        validation_files += filepath[thr1:thr2]\n",
    "        training_files += filepath[thr2:]\n",
    "        print(\"There are {} records under '{}'.\".format(len(filepath), root))\n",
    "\n",
    "\n",
    "with open(TRAINING_FILES_DESC, 'w') as f:\n",
    "    f.write('\\n'.join(training_files))\n",
    "with open(TESTING_FILES_DESC, 'w') as f:\n",
    "    f.write('\\n'.join(testing_files))\n",
    "with open(VALIDATION_FILES_DESC, 'w') as f:\n",
    "    f.write('\\n'.join(validation_files))\n",
    "\n",
    "len(training_files), len(validation_files), len(testing_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse correct files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 LAT                                \t151.46563\n",
      "01 LONG                               \t-9.89855\n",
      "02 DEPTH                              \t-876\n",
      "03 sigh                               \t0\n",
      "04 sigd                               \t-1\n",
      "05 SID                                \t54710\n",
      "06 pred                               \t-943\n",
      "07 id                                 \t1\n",
      "08 d10                                \t1\n",
      "09 d20                                \t1\n",
      "10 d60                                \t0.991437762579\n",
      "11 age                                \t39.3703424623\n",
      "12 VGG                                \t56.1061734039\n",
      "13 rate                               \t5154.39169575\n",
      "14 sed                                \t63.5567064532\n",
      "15 rough                              \t67.7707684862\n",
      "16 G:T                                \t0.744597200561\n",
      "17 kind                               \tM\n",
      "18 year                               \t2000\n",
      "19 NDP_2.5                            \t223.13974293\n",
      "20 NDP_5                              \t1129.34403636\n",
      "21 NDP_10                             \t5682.76607046\n",
      "22 NDP_30                             \t29711.2701283\n",
      "23 STD_2.5m                           \t129.895390742\n",
      "24 STD_5m                             \t294.16470537\n",
      "25 STD_10m                            \t557.675535108\n",
      "26 STD_30m                            \t760.18723492\n",
      "27 MED_2.5m                           \t-1124.45725057\n",
      "28 MED_5m                             \t-1243.61437728\n",
      "29 MED_10m                            \t-2054.68489678\n",
      "30 MED_30m                            \t-1861.12574356\n",
      "31 DEPTH-MED_2.5m)/STD_2.5m           \t1.91275\n",
      "32 (DEPTH-MED_5m)/STD_5m              \t1.24969\n",
      "33 (DEPTH-MED_10m)/STD_10m            \t2.11357\n",
      "34 (DEPTH-MED_30m)/STD_30m            \t1.2959\n"
     ]
    }
   ],
   "source": [
    "with open(TRAINING_FILES_DESC) as f:\n",
    "    training_files = f.readlines()\n",
    "with open(TESTING_FILES_DESC) as f:\n",
    "    testing_files = f.readlines()\n",
    "\n",
    "filename = training_files[0].strip()\n",
    "with io.open(filename, 'r', newline='\\n') as f:\n",
    "    for a, b in zip(columns, f.readline().split()):\n",
    "        print(\"{0:02d} {1:35s}\\t{2:s}\".format(a[0], a[1], b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code for parsing the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Set KIND_INDEX based on what is printed above\n",
    "KIND_INDEX = 17\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_type = {\n",
    "    \"M\": 1,  # - multibeam\n",
    "    \"G\": 2,  # - grid\n",
    "    \"S\": 3,  # - single beam\n",
    "    \"P\": 4,  # - point measurement\n",
    "}\n",
    "\n",
    "with open(TRAINING_FILES_DESC) as f:\n",
    "    training_files = f.readlines()\n",
    "with open(TESTING_FILES_DESC) as f:\n",
    "    testing_files = f.readlines()\n",
    "\n",
    "removed_features = [3, 4, 5, 7]\n",
    "get_label = lambda cols: cols[4] == '9999'\n",
    "training_features = []\n",
    "for filename in training_files:\n",
    "    filename = filename.strip()\n",
    "    if not filename:\n",
    "        continue\n",
    "    features = []\n",
    "    labels = []\n",
    "    with io.open(filename, 'r', newline='\\n') as fread:\n",
    "        for line in fread:\n",
    "            cols = line.strip().split()\n",
    "            if not cols:\n",
    "                continue\n",
    "            cols[KIND_INDEX] = data_type[cols[KIND_INDEX]]\n",
    "            labels.append(get_label(cols))\n",
    "            features.append(np.array(\n",
    "                [float(cols[i]) for i in range(len(cols)) if i not in removed_features]\n",
    "            ))\n",
    "    training_features.append(np.array(features))\n",
    "    if len(training_features) > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 LAT                                \t151.46563\n",
      "01 LONG                               \t-9.89855\n",
      "02 DEPTH                              \t-876\n",
      "03 pred                               \t-943\n",
      "04 d10                                \t1\n",
      "05 d20                                \t1\n",
      "06 d60                                \t0.991437762579\n",
      "07 age                                \t39.3703424623\n",
      "08 VGG                                \t56.1061734039\n",
      "09 rate                               \t5154.39169575\n",
      "10 sed                                \t63.5567064532\n",
      "11 rough                              \t67.7707684862\n",
      "12 G:T                                \t0.744597200561\n",
      "13 kind                               \tM\n",
      "14 year                               \t2000\n",
      "15 NDP_2.5                            \t223.13974293\n",
      "16 NDP_5                              \t1129.34403636\n",
      "17 NDP_10                             \t5682.76607046\n",
      "18 NDP_30                             \t29711.2701283\n",
      "19 STD_2.5m                           \t129.895390742\n",
      "20 STD_5m                             \t294.16470537\n",
      "21 STD_10m                            \t557.675535108\n",
      "22 STD_30m                            \t760.18723492\n",
      "23 MED_2.5m                           \t-1124.45725057\n",
      "24 MED_5m                             \t-1243.61437728\n",
      "25 MED_10m                            \t-2054.68489678\n",
      "26 MED_30m                            \t-1861.12574356\n",
      "27 DEPTH-MED_2.5m)/STD_2.5m           \t1.91275\n",
      "28 (DEPTH-MED_5m)/STD_5m              \t1.24969\n",
      "29 (DEPTH-MED_10m)/STD_10m            \t2.11357\n",
      "30 (DEPTH-MED_30m)/STD_30m            \t1.2959\n"
     ]
    }
   ],
   "source": [
    "with open(TRAINING_FILES_DESC) as f:\n",
    "    training_files = f.readlines()\n",
    "with open(TESTING_FILES_DESC) as f:\n",
    "    testing_files = f.readlines()\n",
    "\n",
    "filename = training_files[0].strip()\n",
    "with io.open(filename, 'r', newline='\\n') as f:\n",
    "    real_index = 0\n",
    "    for a, b in zip(columns, f.readline().split()):\n",
    "        if a[0] in removed_features:\n",
    "            continue\n",
    "        print(\"{0:02d} {1:35s}\\t{2:s}\".format(real_index, a[1], b))\n",
    "        real_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convert data to the LIBSVM format (no longer needed)\n",
    "\n",
    "Rest of the code in this notebook is no longer required for data pre-processing for the bathymetry datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9491"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_files = []\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    all_files += [os.path.join(root, filename)\n",
    "                  for filename in files if filename.endswith(\"libsvm\")]\n",
    "\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in all_files:\n",
    "    split = filename.rsplit(\"/\", 1)\n",
    "    new_dir = split[0] + \"_libsvm/\"\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    os.rename(filename, new_dir + split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"merge-files.sh\", 'w') as f:\n",
    "    for root, subdirs, files in os.walk(\"./\"):\n",
    "        t = [os.path.join(root, filename)\n",
    "             for filename in files if filename.endswith(\"libsvm\")]\n",
    "        if t:\n",
    "            command = \"cat %s > %s/data.libsvm\" % (' '.join(t), root)\n",
    "            f.write(command + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed individual files\n",
    "\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\"libsvm\") and filename != \"data.libsvm\":\n",
    "            os.remove(os.path.join(root, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_files = []\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    all_files += [os.path.join(root, filename)\n",
    "                  for filename in files if filename == \"data.libsvm\"]\n",
    "\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from random import shuffle\n",
    "\n",
    "def shuffle_limited_memory(filename, ntest, nparts):\n",
    "    assert(ntest < nparts)\n",
    "    subfiles = [filename + \"_part%d\" % i for i in range(nparts)]\n",
    "    handlers = [open(name, 'w') for name in subfiles]\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            t = randint(0, nparts - 1)\n",
    "            handlers[t].write(line)\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "\n",
    "    base = filename.rsplit(\"/\", 1)[0]\n",
    "    training = open(base + \"/training.libsvm\", 'w')\n",
    "    testing = open(base + \"/testing.libsvm\", 'w')\n",
    "    shuffle(subfiles)\n",
    "    for i, name in enumerate(subfiles):\n",
    "        with open(name) as f:\n",
    "            lines = f.readlines()        \n",
    "        os.remove(name)\n",
    "        shuffle(lines)\n",
    "        s = ''.join(lines)\n",
    "        if not s.endswith('\\n'):\n",
    "            s += '\\n'\n",
    "        if i < ntest:\n",
    "            testing.write(s)\n",
    "        else:\n",
    "            training.write(s)\n",
    "    training.close()\n",
    "    testing.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./NGA_libsvm/data.libsvm\n",
      "./lakes_libsvm/data.libsvm\n",
      "./GEOMAR_libsvm/data.libsvm\n",
      "./NGA2_libsvm/data.libsvm\n",
      "./JAMSTEC_libsvm/data.libsvm\n",
      "./GEBCO_libsvm/data.libsvm\n",
      "./NOAA_libsvm/data.libsvm\n",
      "./CCOM_libsvm/data.libsvm\n",
      "./US_multi_libsvm/data.libsvm\n",
      "./SIO_libsvm/data.libsvm\n",
      "./3DGBR_libsvm/data.libsvm\n",
      "./NAVO_libsvm/data.libsvm\n",
      "./IFREMER_libsvm/data.libsvm\n",
      "./AGSO_libsvm/data.libsvm\n",
      "./NOAA_geodas_libsvm/data.libsvm\n",
      "./NGDC_libsvm/data.libsvm\n",
      "./IBCAO_libsvm/data.libsvm\n"
     ]
    }
   ],
   "source": [
    "for filename in all_files:\n",
    "    print(filename)\n",
    "    shuffle_limited_memory(filename, 10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in all_files:\n",
    "    old_dir, fname = filename.rsplit(\"/\", 1)\n",
    "    new_dir = old_dir + \"_data\"\n",
    "    os.mkdir(new_dir)\n",
    "    os.rename(filename, os.path.join(new_dir, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"upload-s3.sh\", 'w') as f:\n",
    "    for root, subdirs, files in os.walk(\"./\"):\n",
    "        if root.endswith(\"_libsvm\"):\n",
    "            dirname = root[2:]\n",
    "            f.write(\"aws s3 cp {} s3://tmsn-data/bathymetry/{}/ --recursive\\n\".format(root, dirname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
