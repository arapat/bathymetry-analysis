{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def is_final_tsv_file(filename):\n",
    "    return filename.endswith(\"tsv\")\n",
    "\n",
    "\n",
    "all_files = []\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    all_files += [os.path.join(root, filename)\n",
    "                  for filename in files if is_final_tsv_file(filename)]\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. longitude ',\n",
       " '2. latitude ',\n",
       " '3. depth ',\n",
       " '4. sigh ',\n",
       " '5. sigd ',\n",
       " '6. SID ',\n",
       " '7. predicted_depth ',\n",
       " '8. ID ',\n",
       " '9. d10 ',\n",
       " '10. d20 ',\n",
       " '11. d60 ',\n",
       " '12. seafloor_age ',\n",
       " '13. curvature(VGG)',\n",
       " '14. spreading_rate ',\n",
       " '15. sediment_thickness ',\n",
       " '16. seafloor_roughness ',\n",
       " '17. NDP_@2.5am ',\n",
       " '18. NDP_@5am ',\n",
       " '19. NDP_@10am ',\n",
       " '20. NDP_@30am ',\n",
       " '21. STD_@2.5am ',\n",
       " '22. STD_@5am ',\n",
       " '23. STD_@10am ',\n",
       " '24. STD_@30am ',\n",
       " '25. depth_SUB_median@2.5am ',\n",
       " '26. depth_SUB_median@5am ',\n",
       " '27. depth_SUB_median@10am ',\n",
       " '28. depth_SUB_median@30am ',\n",
       " '29. year ',\n",
       " '30. data_type']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = \"\"\"\n",
    "1. longitude \n",
    "2. latitude \n",
    "3. depth \n",
    "4. sigh \n",
    "5. sigd \n",
    "6. SID \n",
    "7. predicted_depth \n",
    "8. ID \n",
    "9. d10 \n",
    "10. d20 \n",
    "11. d60 \n",
    "12. seafloor_age \n",
    "13. curvature(VGG)\n",
    "14. spreading_rate \n",
    "15. sediment_thickness \n",
    "16. seafloor_roughness \n",
    "17. NDP_@2.5am \n",
    "18. NDP_@5am \n",
    "19. NDP_@10am \n",
    "20. NDP_@30am \n",
    "21. STD_@2.5am \n",
    "22. STD_@5am \n",
    "23. STD_@10am \n",
    "24. STD_@30am \n",
    "25. depth_SUB_median@2.5am \n",
    "26. depth_SUB_median@5am \n",
    "27. depth_SUB_median@10am \n",
    "28. depth_SUB_median@30am \n",
    "29. year \n",
    "30. data_type\n",
    "\"\"\"\n",
    "\n",
    "columns = columns.strip().split('\\n')\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in all_files:\n",
    "    svm_fn = filename.rsplit(\".\", 1)[0] + \".libsvm\"\n",
    "    with open(filename) as fread:\n",
    "        with open(svm_fn, 'w') as fwrite:\n",
    "            for line in fread:\n",
    "                cols = line.strip().split()\n",
    "                if not cols:\n",
    "                    continue\n",
    "                label = (cols[4] == '9999')\n",
    "                cols = cols[:4] + cols[5:]\n",
    "                labels = [\"%d:%s\" % (i, v) for i, v in enumerate(cols) if v.lower() != \"nan\" and v != \"0\"]\n",
    "                fwrite.write(\"%d %s\\n\" % (label, ' '.join(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9491"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_files = []\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    all_files += [os.path.join(root, filename)\n",
    "                  for filename in files if filename.endswith(\"libsvm\")]\n",
    "\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in all_files:\n",
    "    split = filename.rsplit(\"/\", 1)\n",
    "    new_dir = split[0] + \"_libsvm/\"\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    os.rename(filename, new_dir + split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"merge-files.sh\", 'w') as f:\n",
    "    for root, subdirs, files in os.walk(\"./\"):\n",
    "        t = [os.path.join(root, filename)\n",
    "             for filename in files if filename.endswith(\"libsvm\")]\n",
    "        if t:\n",
    "            command = \"cat %s > %s/data.libsvm\" % (' '.join(t), root)\n",
    "            f.write(command + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed individual files\n",
    "\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\"libsvm\") and filename != \"data.libsvm\":\n",
    "            os.remove(os.path.join(root, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_files = []\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    all_files += [os.path.join(root, filename)\n",
    "                  for filename in files if filename == \"data.libsvm\"]\n",
    "\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from random import shuffle\n",
    "\n",
    "def shuffle_limited_memory(filename, ntest, nparts):\n",
    "    assert(ntest < nparts)\n",
    "    subfiles = [filename + \"_part%d\" % i for i in range(nparts)]\n",
    "    handlers = [open(name, 'w') for name in subfiles]\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            t = randint(0, nparts - 1)\n",
    "            handlers[t].write(line)\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "\n",
    "    base = filename.rsplit(\"/\", 1)[0]\n",
    "    training = open(base + \"/training.libsvm\", 'w')\n",
    "    testing = open(base + \"/testing.libsvm\", 'w')\n",
    "    shuffle(subfiles)\n",
    "    for i, name in enumerate(subfiles):\n",
    "        with open(name) as f:\n",
    "            lines = f.readlines()        \n",
    "        os.remove(name)\n",
    "        shuffle(lines)\n",
    "        s = ''.join(lines)\n",
    "        if not s.endswith('\\n'):\n",
    "            s += '\\n'\n",
    "        if i < ntest:\n",
    "            testing.write(s)\n",
    "        else:\n",
    "            training.write(s)\n",
    "    training.close()\n",
    "    testing.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./NGA_libsvm/data.libsvm\n",
      "./lakes_libsvm/data.libsvm\n",
      "./GEOMAR_libsvm/data.libsvm\n",
      "./NGA2_libsvm/data.libsvm\n",
      "./JAMSTEC_libsvm/data.libsvm\n",
      "./GEBCO_libsvm/data.libsvm\n",
      "./NOAA_libsvm/data.libsvm\n",
      "./CCOM_libsvm/data.libsvm\n",
      "./US_multi_libsvm/data.libsvm\n",
      "./SIO_libsvm/data.libsvm\n",
      "./3DGBR_libsvm/data.libsvm\n",
      "./NAVO_libsvm/data.libsvm\n",
      "./IFREMER_libsvm/data.libsvm\n",
      "./AGSO_libsvm/data.libsvm\n",
      "./NOAA_geodas_libsvm/data.libsvm\n",
      "./NGDC_libsvm/data.libsvm\n",
      "./IBCAO_libsvm/data.libsvm\n"
     ]
    }
   ],
   "source": [
    "for filename in all_files:\n",
    "    print(filename)\n",
    "    shuffle_limited_memory(filename, 10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in all_files:\n",
    "    old_dir, fname = filename.rsplit(\"/\", 1)\n",
    "    new_dir = old_dir + \"_data\"\n",
    "    os.mkdir(new_dir)\n",
    "    os.rename(filename, os.path.join(new_dir, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"upload-s3.sh\", 'w') as f:\n",
    "    for root, subdirs, files in os.walk(\"./\"):\n",
    "        if root.endswith(\"_libsvm\"):\n",
    "            dirname = root[2:]\n",
    "            f.write(\"aws s3 cp {} s3://tmsn-data/bathymetry/{}/ --recursive\\n\".format(root, dirname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
