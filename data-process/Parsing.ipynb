{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Validating and formatting\n",
    "\n",
    "These are the scripts that validate and format the raw data files for the bathymetry project.\n",
    "\n",
    "## Meaning of columns\n",
    "\n",
    "\n",
    "```\n",
    "LAT LONG DEPTH sigh sigd SID  pred id   d10   d20   d60    age   VGG   rate  sed    rough\n",
    "C1  C2   C3    C4   C5   C6   C7   C8   C9    C10   C11    C12   C13   C14   C15    C16\n",
    " \n",
    "G:T     kind   year\n",
    "C17     C18    C19\n",
    "\n",
    "NDP_2.5   NDP_5   NDP_10   NDP_30   STD_2.5m   STD_5m   STD_10m   STD_30m   MED_2.5m   MED_5m   MED_10m\n",
    "C20       C21     C22      C23      C24        C25      C26       C27       C28        C29      C30\n",
    "\n",
    "MED_30m\n",
    "C31\n",
    "\n",
    "DEPTH-MED_2.5m)/STD_2.5m     (DEPTH-MED_5m)/STD_5m       (DEPTH-MED_10m)/STD_10m      (DEPTH-MED_30m)/STD_30m\n",
    "C32                          C33                         C34                          C35\n",
    "\n",
    "# NEW FEATURES - 1130\n",
    "\n",
    "## C1   C2   C3      C4    C5     C6    C7    C8    C9\n",
    "## lon   lat   depth  sigh  sigd   SID   pred  ID    (pred-depth)/depth\n",
    "##\n",
    "## C10  C11  C12    C13  C14   C15   C16   C17              C18\n",
    "## d10  d20  d60     age   VGG  rate   sed    roughness    G:T\n",
    "##\n",
    "## C19            C20         C21           C22\n",
    "## NDP2.5m   NDP5m   NDP10m   NDP30m\n",
    "##\n",
    "## C23           C24        C25           C26\n",
    "## STD2.5m  STD5m   STD10m   STD30m\n",
    "##\n",
    "## C27            C28           C29           C30\n",
    "## MED2.5m   MED5m    MED10m   MED30m\n",
    "##\n",
    "## C31                               C32                         C33                              C34\n",
    "## D-MED2.5m/STD2.5m  D-MED5m/STD5m  D-MED10m/STD10m  D-MED30m/STD30m\n",
    "##\n",
    "## C35    C36\n",
    "## year   kind\n",
    "\n",
    "\n",
    "VGG = Vertical gravity gradient (= curvature of the sea surface) \n",
    "rate = seafloor spreading rate\n",
    "rough = seafloor roughness\n",
    "G:T = Gravity:Topography ratio \n",
    "NDP = Number of data points\n",
    "MED = Median\n",
    "STD = Standard dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_BASE_DIR = \"/cryosat/btozer/CREATE_ML_FEATURES/tsv_all\"\n",
    "WORK_DIR = \"/geosat2/julaiti/new_features_1130\"\n",
    "TRAINING_FILES_DESC = os.path.join(WORK_DIR, \"training_files_desc.txt\")\n",
    "TESTING_FILES_DESC = os.path.join(WORK_DIR, \"testing_files_desc.txt\")\n",
    "VALIDATION_FILES_DESC = os.path.join(WORK_DIR, \"validation_files_desc.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(WORK_DIR, \"valid_regions.txt\")) as f:\n",
    "    input_dir = list(map(\n",
    "        lambda s: os.path.join(DATA_BASE_DIR, s.strip()), f.readlines()))\n",
    "len(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tlon\n",
      "1\tlat\n",
      "2\tdepth\n",
      "3\tsigh\n",
      "4\tsigd\n",
      "5\tSID\n",
      "6\tpred\n",
      "7\tID\n",
      "8\t(pred-depth)/depth\n",
      "9\td10\n",
      "10\td20\n",
      "11\td60\n",
      "12\tage\n",
      "13\tVGG\n",
      "14\trate\n",
      "15\tsed\n",
      "16\troughness\n",
      "17\tG:T\n",
      "18\tNDP2.5m\n",
      "19\tNDP5m\n",
      "20\tNDP10m\n",
      "21\tNDP30m\n",
      "22\tSTD2.5m\n",
      "23\tSTD5m\n",
      "24\tSTD10m\n",
      "25\tSTD30m\n",
      "26\tMED2.5m\n",
      "27\tMED5m\n",
      "28\tMED10m\n",
      "29\tMED30m\n",
      "30\tD-MED2.5m/STD2.5m\n",
      "31\tD-MED5m/STD5m\n",
      "32\tD-MED10m/STD10m\n",
      "33\tD-MED30m/STD30m\n",
      "34\tyear\n",
      "35\tkind\n"
     ]
    }
   ],
   "source": [
    "columns = \"\"\"\n",
    "lon   lat   depth  sigh  sigd   SID   pred  ID    (pred-depth)/depth\n",
    "d10  d20  d60     age   VGG  rate   sed    roughness    G:T\n",
    "NDP2.5m   NDP5m   NDP10m   NDP30m\n",
    "STD2.5m  STD5m   STD10m   STD30m\n",
    "MED2.5m   MED5m    MED10m   MED30m\n",
    "D-MED2.5m/STD2.5m  D-MED5m/STD5m  D-MED10m/STD10m  D-MED30m/STD30m\n",
    "year kind\n",
    "\"\"\"\n",
    "\n",
    "columns = list(enumerate(map(lambda s: s.strip(), columns.strip().split())))\n",
    "for a, b in columns:\n",
    "    print(\"{}\\t{}\".format(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of cruises from each regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/3DGBR'. They will *only* be used for testing.\n",
      "There are 127 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/AGSO'.\n",
      "There are 0 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/DNC'. They will *only* be used for testing.\n",
      "There are 3 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/IBCAO'. They will *only* be used for testing.\n",
      "There are 0 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/IFREMER'. They will *only* be used for testing.\n",
      "There are 541 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/JAMSTEC'.\n",
      "There are 105 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/JAMSTEC2'.\n",
      "There are 1374 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/NGA'.\n",
      "There are 24 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/NGA2'. They will *only* be used for testing.\n",
      "There are 14 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/NOAA'. They will *only* be used for testing.\n",
      "There are 247 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/SIO'.\n",
      "There are 480 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/US_multi'.\n",
      "There are 4079 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/NOAA_geodas'.\n",
      "There are 1033 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/NGDC'.\n",
      "There are 51 records under '/cryosat/btozer/CREATE_ML_FEATURES/tsv_all/CCOM'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5633, 1202, 1243)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import io\n",
    "\n",
    "\n",
    "def is_final_tsv_file(filepath):\n",
    "    # if \"SIO\" in filepath:\n",
    "    #     return filepath.endswith(\".tsv_all_final\")\n",
    "    return filepath.endswith(\".tsv_ky\")\n",
    "\n",
    "\n",
    "def is_enough_cols(filepath):\n",
    "    with io.open(filepath, 'r', newline='\\n') as f:\n",
    "        return len(f.readline().split()) in [len(columns), len(columns) - 2]\n",
    "\n",
    "training_files = []\n",
    "testing_files = []\n",
    "validation_files = []\n",
    "for root in input_dir:\n",
    "    all_files = map(lambda s: os.path.join(root, s), os.listdir(root))\n",
    "    valid_files = filter(is_final_tsv_file, all_files)\n",
    "    valid_files = filter(is_enough_cols, valid_files)\n",
    "\n",
    "    filepath = list(valid_files)\n",
    "    shuffle(filepath)\n",
    "    if len(filepath) <= 50:\n",
    "        print(\"There are {} records under '{}'. They will *only* be used for testing.\".format(len(filepath), root))\n",
    "        testing_files += filepath\n",
    "    else:\n",
    "        thr1 = int(len(filepath) * 0.15)\n",
    "        thr2 = thr1 + thr1  # another 0.2\n",
    "        testing_files += filepath[:thr1]\n",
    "        validation_files += filepath[thr1:thr2]\n",
    "        training_files += filepath[thr2:]\n",
    "        print(\"There are {} records under '{}'.\".format(len(filepath), root))\n",
    "\n",
    "\n",
    "with open(TRAINING_FILES_DESC, 'w') as f:\n",
    "    f.write('\\n'.join(training_files))\n",
    "with open(TESTING_FILES_DESC, 'w') as f:\n",
    "    f.write('\\n'.join(testing_files))\n",
    "with open(VALIDATION_FILES_DESC, 'w') as f:\n",
    "    f.write('\\n'.join(validation_files))\n",
    "\n",
    "len(training_files), len(validation_files), len(testing_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse correct files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 lon                                \t143.92639\n",
      "01 lat                                \t-43.99727\n",
      "02 depth                              \t-4637\n",
      "03 sigh                               \t0\n",
      "04 sigd                               \t-1\n",
      "05 SID                                \t10088\n",
      "06 pred                               \t-4633\n",
      "07 ID                                 \t1\n",
      "08 (pred-depth)/depth                 \t0.000862627\n",
      "09 d10                                \t0.984607124262\n",
      "10 d20                                \t0.972010395656\n",
      "11 d60                                \t0.953167490781\n",
      "12 age                                \t39.3518032149\n",
      "13 VGG                                \t20.9209685261\n",
      "14 rate                               \t1773.1538453\n",
      "15 sed                                \t1002.69584759\n",
      "16 roughness                          \t23.1643450107\n",
      "17 G:T                                \t0.58473963179\n",
      "18 NDP2.5m                            \t352.591278239\n",
      "19 NDP5m                              \t1227.86403676\n",
      "20 NDP10m                             \t4867.36187959\n",
      "21 NDP30m                             \t28191.8030442\n",
      "22 STD2.5m                            \t22.2740933357\n",
      "23 STD5m                              \t42.2348361999\n",
      "24 STD10m                             \t86.5628813895\n",
      "25 STD30m                             \t188.407867174\n",
      "26 MED2.5m                            \t-17.3113\n",
      "27 MED5m                              \t-30.9435\n",
      "28 MED10m                             \t-0.9137\n",
      "29 MED30m                             \t1.9221\n",
      "30 D-MED2.5m/STD2.5m                  \t-0.777193\n",
      "31 D-MED5m/STD5m                      \t-0.732653\n",
      "32 D-MED10m/STD10m                    \t-0.0105553\n",
      "33 D-MED30m/STD30m                    \t0.0102018\n",
      "34 year                               \t2000\n",
      "35 kind                               \tG\n"
     ]
    }
   ],
   "source": [
    "with open(TRAINING_FILES_DESC) as f:\n",
    "    training_files = f.readlines()\n",
    "with open(TESTING_FILES_DESC) as f:\n",
    "    testing_files = f.readlines()\n",
    "\n",
    "filename = training_files[0].strip()\n",
    "with io.open(filename, 'r', newline='\\n') as f:\n",
    "    for a, b in zip(columns, f.readline().split()):\n",
    "        print(\"{0:02d} {1:35s}\\t{2:s}\".format(a[0], a[1], b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code for parsing the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Set KIND_INDEX based on what is printed above\n",
    "KIND_INDEX = 35\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_type = {\n",
    "    \"M\": 1,  # - multibeam\n",
    "    \"G\": 2,  # - grid\n",
    "    \"S\": 3,  # - single beam\n",
    "    \"P\": 4,  # - point measurement\n",
    "    \"nan\": np.nan\n",
    "}\n",
    "\n",
    "with open(TRAINING_FILES_DESC) as f:\n",
    "    training_files = f.readlines()\n",
    "with open(TESTING_FILES_DESC) as f:\n",
    "    testing_files = f.readlines()\n",
    "\n",
    "removed_features = [3, 4, 5, 7]\n",
    "get_label = lambda cols: cols[4] == '9999'\n",
    "training_features = []\n",
    "for filename in training_files:\n",
    "    filename = filename.strip()\n",
    "    if not filename:\n",
    "        continue\n",
    "    features = []\n",
    "    labels = []\n",
    "    with io.open(filename, 'r', newline='\\n') as fread:\n",
    "        for line in fread:\n",
    "            cols = line.strip().split()\n",
    "            if len(cols) not in [34, 36]:\n",
    "                continue\n",
    "            if len(cols) == 34:\n",
    "                cols = [\"nan\"] * 2\n",
    "            cols[KIND_INDEX] = data_type[cols[KIND_INDEX]]\n",
    "            labels.append(get_label(cols))\n",
    "            features.append(np.array(\n",
    "                [float(cols[i]) for i in range(len(cols)) if i not in removed_features]\n",
    "            ))\n",
    "    training_features.append(np.array(features))\n",
    "    if len(training_features) > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 lon                                \t143.92639\n",
      "01 lat                                \t-43.99727\n",
      "02 depth                              \t-4637\n",
      "03 pred                               \t-4633\n",
      "04 (pred-depth)/depth                 \t0.000862627\n",
      "05 d10                                \t0.984607124262\n",
      "06 d20                                \t0.972010395656\n",
      "07 d60                                \t0.953167490781\n",
      "08 age                                \t39.3518032149\n",
      "09 VGG                                \t20.9209685261\n",
      "10 rate                               \t1773.1538453\n",
      "11 sed                                \t1002.69584759\n",
      "12 roughness                          \t23.1643450107\n",
      "13 G:T                                \t0.58473963179\n",
      "14 NDP2.5m                            \t352.591278239\n",
      "15 NDP5m                              \t1227.86403676\n",
      "16 NDP10m                             \t4867.36187959\n",
      "17 NDP30m                             \t28191.8030442\n",
      "18 STD2.5m                            \t22.2740933357\n",
      "19 STD5m                              \t42.2348361999\n",
      "20 STD10m                             \t86.5628813895\n",
      "21 STD30m                             \t188.407867174\n",
      "22 MED2.5m                            \t-17.3113\n",
      "23 MED5m                              \t-30.9435\n",
      "24 MED10m                             \t-0.9137\n",
      "25 MED30m                             \t1.9221\n",
      "26 D-MED2.5m/STD2.5m                  \t-0.777193\n",
      "27 D-MED5m/STD5m                      \t-0.732653\n",
      "28 D-MED10m/STD10m                    \t-0.0105553\n",
      "29 D-MED30m/STD30m                    \t0.0102018\n",
      "30 year                               \t2000\n",
      "31 kind                               \tG\n"
     ]
    }
   ],
   "source": [
    "with open(TRAINING_FILES_DESC) as f:\n",
    "    training_files = f.readlines()\n",
    "with open(TESTING_FILES_DESC) as f:\n",
    "    testing_files = f.readlines()\n",
    "\n",
    "filename = training_files[0].strip()\n",
    "with io.open(filename, 'r', newline='\\n') as f:\n",
    "    real_index = 0\n",
    "    for a, b in zip(columns, f.readline().split()):\n",
    "        if a[0] in removed_features:\n",
    "            continue\n",
    "        print(\"{0:02d} {1:35s}\\t{2:s}\".format(real_index, a[1], b))\n",
    "        real_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convert data to the LIBSVM format (no longer needed)\n",
    "\n",
    "Rest of the code in this notebook is no longer required for data pre-processing for the bathymetry datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9491"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_files = []\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    all_files += [os.path.join(root, filename)\n",
    "                  for filename in files if filename.endswith(\"libsvm\")]\n",
    "\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in all_files:\n",
    "    split = filename.rsplit(\"/\", 1)\n",
    "    new_dir = split[0] + \"_libsvm/\"\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    os.rename(filename, new_dir + split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"merge-files.sh\", 'w') as f:\n",
    "    for root, subdirs, files in os.walk(\"./\"):\n",
    "        t = [os.path.join(root, filename)\n",
    "             for filename in files if filename.endswith(\"libsvm\")]\n",
    "        if t:\n",
    "            command = \"cat %s > %s/data.libsvm\" % (' '.join(t), root)\n",
    "            f.write(command + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed individual files\n",
    "\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\"libsvm\") and filename != \"data.libsvm\":\n",
    "            os.remove(os.path.join(root, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_files = []\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    all_files += [os.path.join(root, filename)\n",
    "                  for filename in files if filename == \"data.libsvm\"]\n",
    "\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from random import shuffle\n",
    "\n",
    "def shuffle_limited_memory(filename, ntest, nparts):\n",
    "    assert(ntest < nparts)\n",
    "    subfiles = [filename + \"_part%d\" % i for i in range(nparts)]\n",
    "    handlers = [open(name, 'w') for name in subfiles]\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            t = randint(0, nparts - 1)\n",
    "            handlers[t].write(line)\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "\n",
    "    base = filename.rsplit(\"/\", 1)[0]\n",
    "    training = open(base + \"/training.libsvm\", 'w')\n",
    "    testing = open(base + \"/testing.libsvm\", 'w')\n",
    "    shuffle(subfiles)\n",
    "    for i, name in enumerate(subfiles):\n",
    "        with open(name) as f:\n",
    "            lines = f.readlines()        \n",
    "        os.remove(name)\n",
    "        shuffle(lines)\n",
    "        s = ''.join(lines)\n",
    "        if not s.endswith('\\n'):\n",
    "            s += '\\n'\n",
    "        if i < ntest:\n",
    "            testing.write(s)\n",
    "        else:\n",
    "            training.write(s)\n",
    "    training.close()\n",
    "    testing.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./NGA_libsvm/data.libsvm\n",
      "./lakes_libsvm/data.libsvm\n",
      "./GEOMAR_libsvm/data.libsvm\n",
      "./NGA2_libsvm/data.libsvm\n",
      "./JAMSTEC_libsvm/data.libsvm\n",
      "./GEBCO_libsvm/data.libsvm\n",
      "./NOAA_libsvm/data.libsvm\n",
      "./CCOM_libsvm/data.libsvm\n",
      "./US_multi_libsvm/data.libsvm\n",
      "./SIO_libsvm/data.libsvm\n",
      "./3DGBR_libsvm/data.libsvm\n",
      "./NAVO_libsvm/data.libsvm\n",
      "./IFREMER_libsvm/data.libsvm\n",
      "./AGSO_libsvm/data.libsvm\n",
      "./NOAA_geodas_libsvm/data.libsvm\n",
      "./NGDC_libsvm/data.libsvm\n",
      "./IBCAO_libsvm/data.libsvm\n"
     ]
    }
   ],
   "source": [
    "for filename in all_files:\n",
    "    print(filename)\n",
    "    shuffle_limited_memory(filename, 10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in all_files:\n",
    "    old_dir, fname = filename.rsplit(\"/\", 1)\n",
    "    new_dir = old_dir + \"_data\"\n",
    "    os.mkdir(new_dir)\n",
    "    os.rename(filename, os.path.join(new_dir, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"upload-s3.sh\", 'w') as f:\n",
    "    for root, subdirs, files in os.walk(\"./\"):\n",
    "        if root.endswith(\"_libsvm\"):\n",
    "            dirname = root[2:]\n",
    "            f.write(\"aws s3 cp {} s3://tmsn-data/bathymetry/{}/ --recursive\\n\".format(root, dirname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
