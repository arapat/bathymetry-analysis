{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_BASE_DIR = \"/geosat2/julaiti/tsv_all\"\n",
    "TRAINING_FILES_DESC = os.path.join(DATA_BASE_DIR, \"training_files_desc.txt\")\n",
    "TESTING_FILES_DESC = os.path.join(DATA_BASE_DIR, \"testing_files_desc.txt\")\n",
    "\n",
    "with open(os.path.join(DATA_BASE_DIR, \"valid_regions.txt\")) as f:\n",
    "    input_dir = list(map(\n",
    "        lambda s: os.path.join(DATA_BASE_DIR, s.strip()), f.readlines()))\n",
    "len(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,\n",
       " ['1. longitude',\n",
       "  '2. latitude',\n",
       "  '3. depth',\n",
       "  '4. sigh',\n",
       "  '5. sigd',\n",
       "  '6. SID',\n",
       "  '7. predicted_depth',\n",
       "  '8. ID',\n",
       "  '9. d10',\n",
       "  '10. d20',\n",
       "  '11. d60',\n",
       "  '12. seafloor_age',\n",
       "  '13. curvature(VGG)',\n",
       "  '14. spreading_rate',\n",
       "  '15. sediment_thickness',\n",
       "  '16. seafloor_roughness',\n",
       "  '17. NDP_@2.5am',\n",
       "  '18. NDP_@5am',\n",
       "  '19. NDP_@10am',\n",
       "  '20. NDP_@30am',\n",
       "  '21. STD_@2.5am',\n",
       "  '22. STD_@5am',\n",
       "  '23. STD_@10am',\n",
       "  '24. STD_@30am',\n",
       "  '25. depth_SUB_median@2.5am',\n",
       "  '26. depth_SUB_median@5am',\n",
       "  '27. depth_SUB_median@10am',\n",
       "  '28. depth_SUB_median@30am',\n",
       "  '29. year',\n",
       "  '30. data_type'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = \"\"\"\n",
    "1. longitude \n",
    "2. latitude \n",
    "3. depth \n",
    "4. sigh \n",
    "5. sigd \n",
    "6. SID \n",
    "7. predicted_depth \n",
    "8. ID \n",
    "9. d10 \n",
    "10. d20 \n",
    "11. d60 \n",
    "12. seafloor_age \n",
    "13. curvature(VGG)\n",
    "14. spreading_rate \n",
    "15. sediment_thickness \n",
    "16. seafloor_roughness \n",
    "17. NDP_@2.5am \n",
    "18. NDP_@5am \n",
    "19. NDP_@10am \n",
    "20. NDP_@30am \n",
    "21. STD_@2.5am \n",
    "22. STD_@5am \n",
    "23. STD_@10am \n",
    "24. STD_@30am \n",
    "25. depth_SUB_median@2.5am \n",
    "26. depth_SUB_median@5am \n",
    "27. depth_SUB_median@10am \n",
    "28. depth_SUB_median@30am \n",
    "29. year \n",
    "30. data_type\n",
    "\"\"\"\n",
    "\n",
    "columns = list(map(lambda s: s.strip(), columns.strip().split('\\n')))\n",
    "len(columns), columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16 records under '/geosat2/julaiti/tsv_all/DNC'. They will *only* be used for testing.\n",
      "There are 3 records under '/geosat2/julaiti/tsv_all/IBCAO'. They will *only* be used for testing.\n",
      "There are 24 records under '/geosat2/julaiti/tsv_all/NGA2'. They will *only* be used for testing.\n",
      "There are 14 records under '/geosat2/julaiti/tsv_all/NOAA'. They will *only* be used for testing.\n",
      "There are 50 records under '/geosat2/julaiti/tsv_all/CCOM'. They will *only* be used for testing.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6942, 2426)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def is_final_tsv_file(filepath):\n",
    "    if \"SIO\" in filepath:\n",
    "        return filepath.endswith(\".tsv_all_final\")\n",
    "    return filepath.endswith(\".tsv_all\")\n",
    "\n",
    "\n",
    "training_files = []\n",
    "testing_files = []\n",
    "for root in input_dir:\n",
    "    all_files = map(lambda s: os.path.join(root, s), os.listdir(root))\n",
    "    valid_files = filter(is_final_tsv_file, all_files)\n",
    "    \n",
    "    filepath = list(valid_files)\n",
    "    shuffle(filepath)\n",
    "    if len(filepath) <= 50:\n",
    "        print(\"There are {} records under '{}'. They will *only* be used for testing.\".format(len(filepath), root))\n",
    "        testing_files += filepath\n",
    "    else:\n",
    "        thr = int(len(filepath) * 0.75)\n",
    "        training_files += filepath[:thr]\n",
    "        testing_files += filepath[thr:]\n",
    "\n",
    "with open(TRAINING_FILES_DESC, 'w') as f:\n",
    "    f.write('\\n'.join(training_files))\n",
    "with open(TESTING_FILES_DESC, 'w') as f:\n",
    "    f.write('\\n'.join(testing_files))\n",
    "len(training_files), len(testing_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cols = {}\n",
    "for filename in training_files + testing_files:\n",
    "    with open(filename) as f:\n",
    "        num_cols = len(f.readline().split())\n",
    "    region = filename.rsplit('/', 2)[1]\n",
    "    if region not in name_cols:\n",
    "        name_cols[region] = {}\n",
    "    if num_cols not in name_cols[region]:\n",
    "        name_cols[region][num_cols] = []\n",
    "    name_cols[region][num_cols].append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3DGBR\n",
      "AGSO\n",
      "IFREMER\n",
      "JAMSTEC\n",
      "JAMSTEC2\n",
      "NGA\n",
      "SIO\n",
      "US_multi\n",
      "NOAA_geodas\n",
      "NGDC\n",
      "DNC\n",
      "IBCAO\n",
      "NGA2\n",
      "NOAA\n",
      "CCOM\n"
     ]
    }
   ],
   "source": [
    "rerun = []\n",
    "for region in name_cols:\n",
    "    print(region)\n",
    "    for a, b in name_cols[region].items():\n",
    "        # print(\"{}({})\".format(a, len(b)), end='\\t')\n",
    "        if a != 30:\n",
    "            rerun += b\n",
    "    # print('\\n' + '-' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the files with less than 30 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wrong-col-nums.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(rerun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun = [filename.strip() for filename in rerun]\n",
    "training_files = [filename.strip() for filename in training_files if filename.strip() not in rerun]\n",
    "testing_files =  [filename.strip() for filename in testing_files if filename.strip() not in rerun]\n",
    "\n",
    "with open(TRAINING_FILES_DESC, 'w') as f:\n",
    "    f.write('\\n'.join(training_files))\n",
    "with open(TESTING_FILES_DESC, 'w') as f:\n",
    "    f.write('\\n'.join(testing_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse correct files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. longitude                       \t150.225\n",
      "2. latitude                        \t-9.9\n",
      "3. depth                           \t-1318\n",
      "4. sigh                            \t0\n",
      "5. sigd                            \t-1\n",
      "6. SID                             \t54683\n",
      "7. predicted_depth                 \t-1311\n",
      "8. ID                              \t1\n",
      "9. d10                             \t0.918137582674\n",
      "10. d20                            \t0.772374974666\n",
      "11. d60                            \t0.592828516968\n",
      "12. seafloor_age                   \tNaN\n",
      "13. curvature(VGG)                 \t-49.8216911544\n",
      "14. spreading_rate                 \t4901.890625\n",
      "15. sediment_thickness             \t236.818668\n",
      "16. seafloor_roughness             \t75.0722478379\n",
      "17. NDP_@2.5am                     \t171.559721966\n",
      "18. NDP_@5am                       \t464.000644\n",
      "19. NDP_@10am                      \t2232.07499261\n",
      "20. NDP_@30am                      \t10060.9879654\n",
      "21. STD_@2.5am                     \t20.904548\n",
      "22. STD_@5am                       \t40.77994\n",
      "23. STD_@10am                      \t54.3156827884\n",
      "24. STD_@30am                      \t455.714756334\n",
      "25. depth_SUB_median@2.5am         \t-1.54296875\n",
      "26. depth_SUB_median@5am           \t-62.85546875\n",
      "27. depth_SUB_median@10am          \t-32.56640625\n",
      "28. depth_SUB_median@30am          \t-638.04296875\n",
      "29. year                           \t2000\n",
      "30. data_type                      \tM\n"
     ]
    }
   ],
   "source": [
    "with open(training_files[0]) as f:\n",
    "    for a, b in zip(columns, f.readline().split()):\n",
    "        print(\"{0:35s}\\t{1}\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_FILES_DESC) as f:\n",
    "    training_files = f.readlines()\n",
    "with open(TESTING_FILES_DESC) as f:\n",
    "    testing_files = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_type = {\n",
    "    \"M\": 1,  # - multibeam\n",
    "    \"G\": 2,  # - grid\n",
    "    \"S\": 3,  # - single beam\n",
    "    \"P\": 4,  # - point measurement\n",
    "}\n",
    "\n",
    "with open(TRAINING_FILES_DESC) as f:\n",
    "    training_files = f.readlines()\n",
    "with open(TESTING_FILES_DESC, 'w') as f:\n",
    "    testing_files = f.readlines()\n",
    "\n",
    "removed_features = [0, 1, 3, 4, 5, 7]\n",
    "get_label = lambda cols: cols[4] == '9999'\n",
    "training_features = []\n",
    "for filename in training_files:\n",
    "    filename = filename.strip()\n",
    "    if not filename:\n",
    "        continue\n",
    "    features = []\n",
    "    labels = []\n",
    "    with open(filename) as fread:\n",
    "        for line in fread:\n",
    "            cols = line.strip().split()\n",
    "            if not cols:\n",
    "                continue\n",
    "            cols[29] = data_type[cols[29]]\n",
    "            labels.append(get_label(cols))\n",
    "            features.append(np.array(\n",
    "                [float(cols[i]) for i in range(len(cols)) if i not in removed_features]\n",
    "            ))\n",
    "    training_features.append(np.array(features))\n",
    "    if len(training_features) > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No longer needed\n",
    "\n",
    "Rest of the code in this notebook is no longer required for data pre-processing for the bathymetry datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9491"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_files = []\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    all_files += [os.path.join(root, filename)\n",
    "                  for filename in files if filename.endswith(\"libsvm\")]\n",
    "\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in all_files:\n",
    "    split = filename.rsplit(\"/\", 1)\n",
    "    new_dir = split[0] + \"_libsvm/\"\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    os.rename(filename, new_dir + split[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"merge-files.sh\", 'w') as f:\n",
    "    for root, subdirs, files in os.walk(\"./\"):\n",
    "        t = [os.path.join(root, filename)\n",
    "             for filename in files if filename.endswith(\"libsvm\")]\n",
    "        if t:\n",
    "            command = \"cat %s > %s/data.libsvm\" % (' '.join(t), root)\n",
    "            f.write(command + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed individual files\n",
    "\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\"libsvm\") and filename != \"data.libsvm\":\n",
    "            os.remove(os.path.join(root, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_files = []\n",
    "for root, subdirs, files in os.walk(\"./\"):\n",
    "    all_files += [os.path.join(root, filename)\n",
    "                  for filename in files if filename == \"data.libsvm\"]\n",
    "\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from random import shuffle\n",
    "\n",
    "def shuffle_limited_memory(filename, ntest, nparts):\n",
    "    assert(ntest < nparts)\n",
    "    subfiles = [filename + \"_part%d\" % i for i in range(nparts)]\n",
    "    handlers = [open(name, 'w') for name in subfiles]\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            t = randint(0, nparts - 1)\n",
    "            handlers[t].write(line)\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "\n",
    "    base = filename.rsplit(\"/\", 1)[0]\n",
    "    training = open(base + \"/training.libsvm\", 'w')\n",
    "    testing = open(base + \"/testing.libsvm\", 'w')\n",
    "    shuffle(subfiles)\n",
    "    for i, name in enumerate(subfiles):\n",
    "        with open(name) as f:\n",
    "            lines = f.readlines()        \n",
    "        os.remove(name)\n",
    "        shuffle(lines)\n",
    "        s = ''.join(lines)\n",
    "        if not s.endswith('\\n'):\n",
    "            s += '\\n'\n",
    "        if i < ntest:\n",
    "            testing.write(s)\n",
    "        else:\n",
    "            training.write(s)\n",
    "    training.close()\n",
    "    testing.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./NGA_libsvm/data.libsvm\n",
      "./lakes_libsvm/data.libsvm\n",
      "./GEOMAR_libsvm/data.libsvm\n",
      "./NGA2_libsvm/data.libsvm\n",
      "./JAMSTEC_libsvm/data.libsvm\n",
      "./GEBCO_libsvm/data.libsvm\n",
      "./NOAA_libsvm/data.libsvm\n",
      "./CCOM_libsvm/data.libsvm\n",
      "./US_multi_libsvm/data.libsvm\n",
      "./SIO_libsvm/data.libsvm\n",
      "./3DGBR_libsvm/data.libsvm\n",
      "./NAVO_libsvm/data.libsvm\n",
      "./IFREMER_libsvm/data.libsvm\n",
      "./AGSO_libsvm/data.libsvm\n",
      "./NOAA_geodas_libsvm/data.libsvm\n",
      "./NGDC_libsvm/data.libsvm\n",
      "./IBCAO_libsvm/data.libsvm\n"
     ]
    }
   ],
   "source": [
    "for filename in all_files:\n",
    "    print(filename)\n",
    "    shuffle_limited_memory(filename, 10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in all_files:\n",
    "    old_dir, fname = filename.rsplit(\"/\", 1)\n",
    "    new_dir = old_dir + \"_data\"\n",
    "    os.mkdir(new_dir)\n",
    "    os.rename(filename, os.path.join(new_dir, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"upload-s3.sh\", 'w') as f:\n",
    "    for root, subdirs, files in os.walk(\"./\"):\n",
    "        if root.endswith(\"_libsvm\"):\n",
    "            dirname = root[2:]\n",
    "            f.write(\"aws s3 cp {} s3://tmsn-data/bathymetry/{}/ --recursive\\n\".format(root, dirname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
